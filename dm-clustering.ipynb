{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of clustering approaches\n",
    "\n",
    "Before we begin, let's take a look at the visualization of two main approaches to clustering:\n",
    "- [partition-based methods](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)\n",
    "- [density-based methods](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance metrics\n",
    "\n",
    "A metric is a function $d(x,y)$ which fulfills the following conditions:\n",
    "- symmetry: $d(x,y) = d(y,x)$\n",
    "- identity: $d(x,x) = 0$\n",
    "- triangle inequality: $d(x,z) \\leq d(x,y) + d(y,z)$\n",
    "\n",
    "There are numerous distance metrics in use in data mining. Some of the most popular include:\n",
    "- Euclidean: $d(x,y) = \\sqrt{\\sum\\limits_{i=1}^n (x_i^2 - y_i^2)}$\n",
    "- Manhattan (cityblock): $d(x,y) = \\sum\\limits_{i=1}^n |x_i - y_i|$\n",
    "- Chebyshev (chessboard):  $d(x,y) = \\max_i|x_i - y_i|$\n",
    "- Canberra (ranked list): $d(x,y) = \\sum\\limits_{i=1}^n \\frac{|x_i - y_i|}{|x_i|+|y_i|}$\n",
    "\n",
    "An entire family of metrics is defined as Minkowski distances: $d(x,y,p) = \\sum\\limits_{i=1}^n (x_i^p - y_i^p)^{\\frac{1}{p}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AHC: Agglomerative Hierarchical Clustering\n",
    "\n",
    "In AHC objects are automatically inserted into a taxonomy based on pairwise distances between objects. Given a numerical representation of text paragraphs it is possible to generate taxonomies, assign paragraphs to correct nodes in the taxonomy, to compare two taxonomies, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T09:27:41.616858Z",
     "start_time": "2021-04-28T09:27:41.335742Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_table(\"zoo.tab\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T09:28:11.833719Z",
     "start_time": "2021-04-28T09:28:11.770586Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T09:28:31.765347Z",
     "start_time": "2021-04-28T09:28:31.761096Z"
    }
   },
   "outputs": [],
   "source": [
    "# transform DataFrame to a NumPy matrix\n",
    "ndf = df.iloc[:, 1:-1].to_numpy()\n",
    "\n",
    "ndf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates a hierarchical clustering model based on two parameters: \n",
    "\n",
    "  * __linkage__ defines the way the distance between two clusters is computed\n",
    "  * __metric__ defines the way the distance between two points is computed\n",
    "  \n",
    "Possible values for __linkage__ are: *single*, *complete*, *average*, *weighted*, *centroid*, *ward*, for definitions of these methods [see the SciPy documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html)\n",
    "\n",
    "Possible values for __metric__ include, among others: *euclidean*, *cityblock*, *minkowski*, *cosine*, *hamming*. The full list with definitions can be found [in the SciPy documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html)\n",
    "\n",
    "Run the example below and then try to find the best combination of these parameters, i.e., the combination which produces the hierarchy which is the most similar to our biological understanding of the tree of life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T09:33:14.344726Z",
     "start_time": "2021-04-28T09:33:12.951809Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (20.0, 40.0)\n",
    "\n",
    "model = linkage(ndf, method='complete', metric='euclidean')\n",
    "\n",
    "labels = df.name.values + ' : ' + df.type.values\n",
    "\n",
    "plot  = dendrogram(model, labels=labels, orientation='right', leaf_font_size=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Experiment with different ways to merge clusters and with different distance metric. Each time visually inspect the resulting dendrogram. Try to identify *the worst* cluster merging criterion for this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering\n",
    "\n",
    "K-means clustering produces the partitioning of the space into [Voronoi cells](http://alexbeutel.com/webgl/voronoi.html).\n",
    "\n",
    "The code below illustrates the use of k-means to cluster numerical points. When running the code experiment with varying the number of true clusters generated and the expected number of clusters. See what happens when\n",
    "\n",
    "  * expected number of clusters is smaller/greater than the true number of clusters\n",
    "  * the true number of clusters is small vs large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T09:48:45.208722Z",
     "start_time": "2021-04-28T09:48:44.629363Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "true_number_of_clusters = 15\n",
    "\n",
    "# generate an artificial dataset\n",
    "X, y = make_blobs(n_samples=1000, n_features=3, centers=true_number_of_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T09:48:49.455595Z",
     "start_time": "2021-04-28T09:48:49.405008Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "fig.add_axes(ax)\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T09:49:06.805345Z",
     "start_time": "2021-04-28T09:49:06.758806Z"
    }
   },
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=9)\n",
    "model.fit(X)\n",
    "\n",
    "# assign points to clusters\n",
    "labels = model.predict(X)\n",
    "\n",
    "# print the coordinates of centroids\n",
    "print(model.cluster_centers_)\n",
    "\n",
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T09:49:20.240325Z",
     "start_time": "2021-04-28T09:49:20.205444Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "fig.add_axes(ax)\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, guessing the right number of clusters is important. Below we will experiment with different settings of the number of cluster and try to deduce the best using [the Silhouette score](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T09:54:42.507444Z",
     "start_time": "2021-04-28T09:54:38.842121Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "k_range = range(2,20)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for k in tqdm(k_range):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    score = silhouette_score(X, labels, metric='euclidean')\n",
    "    scores.append((k, score))\n",
    "\n",
    "ax_x, ax_y = zip(*scores)\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = plt.axes()\n",
    "plt.xticks(k_range)\n",
    "plt.grid()\n",
    "plt.plot(ax_x, ax_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Read about the Davies-Bouldin index [here](https://scikit-learn.org/stable/modules/clustering.html#davies-bouldin-index) and [here](https://en.wikipedia.org/wiki/Davies%E2%80%93Bouldin_index). Repeat the experiment with the discovery of the optimal number of clusters for k-Means, this time using DB-index as the optimization criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering of text documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "\n",
    "The main idea is to represent a word as a vector in an $n$ dimensional space (usually $n=300$), in such a way that the proximity of words in this latent vector space indicates semantic proximity of words. So, this representation not only puts synonyms close to each other, it also puts words that are semantically related quite close to each other.\n",
    "\n",
    "In order to run this code you have to install SpaCy and download precomputed word embeddings:\n",
    "\n",
    "```bash\n",
    "bash$ pip install -U spacy\n",
    "bash$ python -m spacy download en_core_web_md\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T10:55:53.243169Z",
     "start_time": "2021-04-28T10:55:51.930775Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words in Spacy have vector representations computed using GloVe (Global Vectors for Word Representation), a method of unsupervised vector computation trained on Wikipedia, Twitter and Common Crawl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T10:55:54.214592Z",
     "start_time": "2021-04-28T10:55:54.185483Z"
    }
   },
   "outputs": [],
   "source": [
    "king = nlp('king')\n",
    "print(king.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T10:55:56.671475Z",
     "start_time": "2021-04-28T10:55:56.615790Z"
    }
   },
   "outputs": [],
   "source": [
    "words = ['king','queen','kingdom','emperor','spades','Lear','milk']\n",
    "    \n",
    "for word in words:\n",
    "    print(f\"{king} {word} \\t {king.similarity(nlp(word))} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the clustering in action let us import a text document and try to find clusters the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T10:59:51.360762Z",
     "start_time": "2021-04-28T10:59:48.601945Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "respond = requests.get(\"https://en.wikipedia.org/wiki/Poznań\")\n",
    "soup = BeautifulSoup(respond.text, \"html\")\n",
    "page = soup.find_all('p')\n",
    "\n",
    "raw_text = ''.join([paragraph.text for paragraph in page])\n",
    "\n",
    "print(raw_text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will use the traditional vector space model, where each phrase will be represented as the vector of words appearing in this phrase. We will use the TF-IDF weighting of words and some stop-word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T11:00:00.688356Z",
     "start_time": "2021-04-28T10:59:59.843474Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "phrases = raw_text.replace('\\n','').split('.')\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(phrases)\n",
    "\n",
    "print('\\nVocabulary')\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print('\\nList of stopwords')\n",
    "print(vectorizer.get_stop_words())\n",
    "\n",
    "print('\\nDocument in the vector space')\n",
    "print(X.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T11:00:07.243741Z",
     "start_time": "2021-04-28T11:00:07.234062Z"
    }
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us verify how efficient is simple k-Means when applied to the simple TF-IDF document representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T11:00:15.803442Z",
     "start_time": "2021-04-28T11:00:15.736821Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "true_k = 10\n",
    "num_words = 5\n",
    "\n",
    "model = KMeans(n_clusters=true_k, max_iter=1000)\n",
    "model.fit(X)\n",
    "\n",
    "centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in tqdm(range(true_k)):\n",
    "    print (f'Cluster {i}: {[terms[j] for j in centroids[i, :num_words]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the next step, we will create an embedding for each sentence. The embedding of a sentence will be the average of word vectors for each word appearing in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T11:00:31.437615Z",
     "start_time": "2021-04-28T11:00:31.430220Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_text = raw_text.replace(',', ' ').replace('(', ' ').replace(')', ' ').replace('\\n','').replace('.',' .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T11:00:45.356433Z",
     "start_time": "2021-04-28T11:00:34.762440Z"
    }
   },
   "outputs": [],
   "source": [
    "words = {\n",
    "    word.lower(): nlp(word).vector\n",
    "    for word in set(raw_text.split())\n",
    "    if np.any(nlp(word).vector)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid \"bad embeddings\" we will use only the sentences which contain at least 5 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack(\n",
    "    [\n",
    "        np.mean([words[w] for w in phrase if w in words], axis=0) \n",
    "        for phrase in phrases \n",
    "        if len(phrase.split()) >= 5\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the number of sentences is similar (284) and each sentence is represented as a 300-dimensional vector.\n",
    "\n",
    "Our next step is to run `DBSCAN` algorithm on these sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T11:06:20.202294Z",
     "start_time": "2021-04-28T11:06:20.021605Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "model = DBSCAN(eps=0.3, min_samples=3)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the labels assigned to each sentence. The value -1 denotes a sentence which is an outlier (it is not the part of any cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_id in np.unique(model.labels_):\n",
    "    if cluster_id >= 0:\n",
    "        print(f\"Cluster {cluster_id}\")\n",
    "        print([phrases[i] for i in np.where(model.labels_ == cluster_id)[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us see how `DBSCAN` handles the word embeddings. This time, we will skip the information about the sentences, the clustering model will be computed directly from the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack(words.values())\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DBSCAN(eps=0.15, min_samples=3)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_id in np.unique(model.labels_):\n",
    "    if cluster_id >= 0:\n",
    "        print(f\"Cluster {cluster_id}\")\n",
    "        print([list(words.keys())[i] for i in np.where(model.labels_ == cluster_id)[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Change the clustering algorithm to `k-Means` and use one of the optimization criteria (either the silhouette index or the Davies-Bouldin index) to find the \"right\" number of clusters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "547.667px",
    "left": "1411px",
    "right": "20px",
    "top": "115px",
    "width": "366.625px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
